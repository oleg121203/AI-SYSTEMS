{
  "mcp_api": "http://localhost:7860",
  "web_port": 7860,
  "log_file": "logs/mcp.log",
  "ai1_max_concurrent_tasks": 10,
  "target": "Create a Retro NES-Style Flash Game with FastAPI\nRole: You are an expert game developer specializing in modern tools (HTML5 + JavaScript, Phaser.js, or Godot). Your task is to create a simple yet addictive flash game inspired by classic NES/Dendy games (8-bit graphics, limited color palette, pixel art).\\n\\nCore Requirements:\\n- Genre: Platformer / Arcade (e.g., Super Mario Bros, Battle City, Contra)\n\\nTech Stack:\\n- Frontend: HTML5 Canvas/WebGL (or frameworks like Phaser.js)\n- Backend (if needed): FastAPI for high-performance endpoints (e.g., leaderboards)\\n- Sound: Chip-tune style (e.g., using jsfxr)\\n\\nGameplay:\\n- Basic mechanics: jumping, enemies, collectible coins\n- 2-3 levels with gradually increasing difficulty\n- Lives system and scoring (save progress via localStorage or FastAPI)\\n\\nRetro Aesthetics:\\n- Resolution: 256x240 pixels (NES-like)\\n- Color palette: 4-6 colors per sprite\n- Tile-based level design\\n\\nBonus Features:\\n- Secret cheat codes (e.g., \"KONAMI code\" for extra lives)\\n- Pause/restart functionality\n- Simple enemy AI (patrol or chase behavior)\\n\\nDeliverables:\\n- Full source code (HTML/JS or chosen framework)\n- Setup instructions (local development + FastAPI deployment if used)\\n- Ideas for scaling (e.g., multiplayer, new levels)\\n\\nAsset Suggestions:\\n- Sprites: OpenGameArt.org (8-bit/16-bit sections)\n- Sound: SFXR tools for NES-style effects\\n\\nNotes for AI:\\n- If using FastAPI, provide a minimal main.py with a /highscores endpoint.\n- Example code for player movement (e.g., velocityX/Y) to help structure the response.\n- Emphasize performance (lightweight JS, no lag on low-end devices).",
  "confidence_threshold": {
    "tests_passed": 0.5,
    "syntax_score": 0.3,
    "readability": 0.2,
    "coverage": 0.3
  },
  "metrics_by_role": {
    "executor": ["syntax_score", "readability"],
    "tester": ["tests_passed", "coverage"],
    "documenter": ["readability"]
  },
  "ai1_prompt": "Your goal is {target}. You are the main curator (AI1), the only one who makes all key decisions to create a working product. The development process is divided into three stages and must run in a continuous loop until completed. NEVER STOP AFTER RECEIVING THE STRUCTURE. ALWAYS CONTINUE TO THE NEXT STEP.\n\n**Stage 1**: First, you must align the project structure with AI3 through debates and discussions to determine the best file structure. Once the structure is agreed upon, it is put into action, and only then are the files created once. Changes to the structure are possible only in critical moments at your discretion.\n\n**Stage 2**: After the files are created, you MUST IMMEDIATELY create subtasks for EACH FILE in the structure and distribute these subtasks to AI2 to generate code, tests, or documentation. First, create executor subtasks for all files. After receiving implementation for a file, create tester and documenter subtasks for the same file.\n\n**Stage 3**: Analyze reports from AI2 and ensure AI3 updates the files with the latest content. Assign testing subtasks to AI2, analyze test results, and make adjustments until the product is complete. The final decision is always yours.\n\nWhen generating subtasks, specify the role as one of: executor, tester, or documenter. For example:\nSubtask text: Implement the main logic\nRole: executor\nFile: main.py\nOnly these roles are allowed.\n\nCONTINUOUSLY MONITOR status and create new subtasks until ALL FILES are implemented, tested and documented.",
  "ai2_prompts": [
    "You are an executor (AI2) who receives subtasks from AI1 to generate the required content for the specified task and file ({filename}) defined by the structure from AI3. Return only the raw file content itself without any additional explanations, comments, or introductory phrases. DO NOT USE MARKDOWN CODE BLOCKS WITH BACKTICKS (```) - return the raw code only. Do not write to files; AI3 handles that. Ensure the generated content is valid and well-formatted.",
    "You are a tester (AI2) who receives subtasks from AI1 to generate unit tests for the specified file defined by the structure from AI3. Return only the test code without additional explanations or markdown formatting. DO NOT USE MARKDOWN CODE BLOCKS WITH BACKTICKS (```) - return the raw code only. Be sure to consider the file name: {filename}. Do not execute tests or write to files; AI3 handles file updates.",
    "You are a documenter (AI2) who receives subtasks from AI1 to generate documentation for the specified file defined by the structure from AI3. Return only the documentation text without additional explanations or markdown formatting. DO NOT USE MARKDOWN CODE BLOCKS WITH BACKTICKS (```) - return the raw code only. Be sure to consider the file name: {filename}. Do not write to files; AI3 handles that."
  ],
  "ai3_prompt": "Your goal is {target}. During the preparation stage, you propose a file structure... After the structure is agreed upon, you create the necessary directories and files. **ABSOLUTELY CRITICAL: When creating files initially, they MUST be COMPLETELY EMPTY (zero bytes, no content, no comments, no placeholders, no boilerplate). This is non-negotiable.** Then, monitor the log files continuously for AI2 reports (marked as '[AI2] Code for file:'). Extract the content provided by AI2. **BEFORE writing, REMOVE any surrounding markdown code fences (like ```python at the start or ``` at the end) from the extracted content.** Write ONLY the cleaned, raw content provided by AI2 to the corresponding file, overwriting existing content. Do not add or modify anything else. Changes to the structure are possible only in critical moments at AI1's decision.",
  "ai_config": {
    "ai1": {
      "providers": ["gemini-pro2.5", "groq", "ollama"],
      "model": "gemini-2.5-pro-preview-03-25",
      "max_tokens": 4096,
      "temperature": 0.7
    },
    "ai2": {
      "providers": {
        "executor": ["gemini-pro2.5", "openrouter-qwen", "groq", "ollama"],
        "tester": ["ollama", "groq", "gemini"],
        "documenter": ["groq", "gemini", "ollama"]
      },
      "max_tokens": 4096,
      "temperature": 0.4
    },
    "ai3": {
      "providers": ["ollama", "gemini", "groq"],
      "model": "qwen3",
      "max_tokens": 2048,
      "temperature": 0.7
    }
  },
  "providers": {
    "ollama": {
      "type": "ollama",
      "endpoint": "http://host.docker.internal:11434",
      "model": "qwen3"
    },
    "ollama1": {
      "type": "ollama",
      "endpoint": "http://46.219.108.236:11434",
      "model": "llama3.2:latest"
    },
    "codestral": {
      "type": "codestral",
      "endpoint": "https://codestral.mistral.ai/v1",
      "model": "codestral-latest"
    },
    "gemini": {
      "type": "gemini",
      "model": "gemini-1.5-flash"
    },
    "gemini-pro2.5": {
      "type": "gemini",
      "model": "gemini-2.5-pro-preview-03-25"
    },
    "cohere": {
      "type": "cohere",
      "endpoint": null,
      "model": "command-r"
    },
    "groq": {
      "type": "groq",
      "endpoint": null,
      "model": "llama3-70b-8192"
    },
    "together": {
      "type": "together",
      "endpoint": null,
      "model": "mistralai/Mixtral-8x7B-Instruct-v0.1"
    },
    "together-maverick": {
      "type": "together",
      "endpoint": null,
      "model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"
    },
    "openrouter": {
      "type": "openrouter",
      "endpoint": "https://openrouter.ai/api/v1",
      "model": "anthropic/claude-3-opus",
      "max_tokens": 1024
    },
    "openrouter-qwen": {
      "type": "openrouter",
      "endpoint": "https://openrouter.ai/api/v1",
      "model": "qwen/qwen-2.5-coder-32b-instruct",
      "max_tokens": 1024
    },
    "openrouter-qwen-2": {
      "type": "openrouter",
      "endpoint": "https://openrouter.ai/api/v1",
      "model": "anthropic/claude-3-sonnet",
      "max_tokens": 1024
    },
    "openrouter-gemini": {
      "type": "openrouter",
      "endpoint": "https://openrouter.ai/api/v1",
      "model": "google/gemini-pro-1.5",
      "max_tokens": 1024
    }
  },
  "request_delays": {
    "ai1": {
      "min": 1.0,
      "max": 3.0
    },
    "ai2": {
      "executor": {
        "min": 0.5,
        "max": 2.0
      },
      "tester": {
        "min": 1.5,
        "max": 4.0
      },
      "documenter": {
        "min": 1.5,
        "max": 4.0
      }
    },
    "ai3": {
      "min": 2.0,
      "max": 5.0
    }
  },
  "desired_active_buffer": 500,
  "ai1_desired_active_buffer": 500,
  "github_repo": "YOUR_GITHUB_USERNAME/YOUR_REPO_NAME",
  "github_actions_check_interval": 60
}
