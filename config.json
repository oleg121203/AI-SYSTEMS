{
    "mcp_api": "http://localhost:7860",
    "web_port": 7860,
    "log_file": "logs/mcp.log",
    "ai1_max_concurrent_tasks": 30,
    "target": "Create a Retro NES-Style Flash Game with FastAPI\nRole: You are an expert game developer specializing in modern tools (HTML5 + JavaScript, Phaser.js, or Godot). Your task is to create a simple yet addictive flash game inspired by classic NES/Dendy games (8-bit graphics, limited color palette, pixel art).\\n\\nCore Requirements:\\n- Genre: Platformer / Arcade (e.g., Super Mario Bros, Battle City, Contra)\\n\\nTech Stack:\\n- Frontend: HTML5 Canvas/WebGL (or frameworks like Phaser.js)\\n- Backend (if needed): FastAPI for high-performance endpoints (e.g., leaderboards)\\n- Sound: Chip-tune style (e.g., using jsfxr)\\n\\nGameplay:\\n- Basic mechanics: jumping, enemies, collectible coins\\n- 2-3 levels with gradually increasing difficulty\\n- Lives system and scoring (save progress via localStorage or FastAPI)\\n\\nRetro Aesthetics:\\n- Resolution: 256x240 pixels (NES-like)\\n- Color palette: 4-6 colors per sprite\\n- Tile-based level design\\n\\nBonus Features:\\n- Secret cheat codes (e.g., \"KONAMI code\" for extra lives)\\n- Pause/restart functionality\\n- Simple enemy AI (patrol or chase behavior)\\n\\nDeliverables:\\n- Full source code (HTML/JS or chosen framework)\\n- Setup instructions (local development + FastAPI deployment if used)\\n- Ideas for scaling (e.g., multiplayer, new levels)\\n\\nAsset Suggestions:\\n- Sprites: OpenGameArt.org (8-bit/16-bit sections)\\n- Sound: SFXR tools for NES-style effects\\n\\nNotes for AI:\\n- If using FastAPI, provide a minimal main.py with a /highscores endpoint.\\n- Example code for player movement (e.g., velocityX/Y) to help structure the response.\\n- Emphasize performance (lightweight JS, no lag on low-end devices).",
    "confidence_threshold": {
        "tests_passed": 0.5,
        "syntax_score": 0.3,
        "readability": 0.2,
        "coverage": 0.3
    },
    "metrics_by_role": {
        "executor": [
            "syntax_score",
            "readability"
        ],
        "tester": [
            "tests_passed",
            "coverage"
        ],
        "documenter": [
            "readability"
        ]
    },
    "ai1_prompt": "Your goal is {target}. You are the main curator (AI1), the only one who makes all key decisions to create a working product. The development process is divided into three stages and must run in a continuous loop until completed. NEVER STOP AFTER RECEIVING THE STRUCTURE. ALWAYS CONTINUE TO THE NEXT STEP.\n\n**Stage 1**: First, you must align the project structure with AI3 through debates and discussions to determine the best file structure. Once the structure is agreed upon, it is put into action, and only then are the files created once. Changes to the structure are possible only in critical moments at your discretion.\n\n**Stage 2**: After the files are created, you MUST IMMEDIATELY create subtasks for EACH FILE in the structure and distribute these subtasks to AI2 to generate code, tests, or documentation. First, create executor subtasks for all files. After receiving implementation for a file, create tester and documenter subtasks for the same file.\n\n**Stage 3**: Analyze reports from AI2 and ensure AI3 updates the files with the latest content. Assign testing subtasks to AI2, analyze test results (including recommendations from AI3), and make adjustments until the product is complete. **You will also receive monitoring reports (about errors, worker status, queue load) and detailed test analysis from AI3 via collaboration; use this information proactively to prioritize tasks, request fixes, and manage the development flow.** The final decision is always yours.\n\nWhen generating subtasks, specify the role as one of: executor, tester, or documenter. For example:\nSubtask text: Implement the main logic\nRole: executor\nFile: main.py\nOnly these roles are allowed.\n\nCONTINUOUSLY MONITOR status and create new subtasks until ALL FILES are implemented, tested and documented.",
    "ai2_prompts": [
        "You are an expert programmer. Create the content for the file {filename} based on the following task description. If you're working with HTML, CSS, JS, or other web technologies, ensure you follow modern best practices and create responsive, well-structured code optimized for performance.",
        "You are a testing expert. Generate unit tests for the code in file {filename}. For web technologies (HTML, CSS, JS, JSX, TSX, Vue), use appropriate testing frameworks like Jest, Testing Library, Cypress, or Playwright. For style files, create visual regression and responsive design tests. For markup, validate structure and accessibility.",
        "You are a technical writer. Generate documentation (e.g., docstrings, comments) for the code in file {filename}. For web components and frontend code, include usage examples, prop descriptions, styling information, and responsive design considerations."
    ],
    "ai3_prompt": "Your primary role is to establish the foundational structure for the project based on the target: \"{target}\". Generate a comprehensive and accurate JSON structure, including all necessary directories and files with appropriate extensions (e.g., .py, .js, .html, .css, .ts, .tsx, .md, Dockerfile, requirements.txt, etc.). Consider common project layouts and best practices for the specified technologies. This initial structure is critical, as you will later monitor the development process based on it, analyze test results, check logs, and report issues related to these files back to AI1. Ensure the structure is logical and complete.",
    "supported_file_extensions": [
        ".py",
        ".js",
        ".ts",
        ".java",
        ".cpp",
        ".hpp",
        ".go",
        ".rs",
        ".php",
        ".html",
        ".htm",
        ".css",
        ".scss",
        ".sass",
        ".less",
        ".jsx",
        ".tsx",
        ".vue",
        ".svelte",
        ".json",
        ".yaml",
        ".yml",
        ".md",
        ".txt"
    ],
    "ai_config": {
        "ai1": {
            "providers": [
                "codestral",
                "openrouter-qwen",
                "gemini-pro2.5",
                "groq",
                "ollama"
            ],
            "model": "qwen3:latest",
            "max_tokens": 2048,
            "temperature": 0.7
        },
        "ai2": {
            "providers": {
                "executor": [
                    "codestral",
                    "codestral2",
                    "groq",
                    "openrouter-qwen",
                    "gemini-pro2.5",
                    "ollama"
                ],
                "tester": [
                    "codestral",
                    "codestral2",
                    "groq",
                    "gemini"
                ],
                "documenter": [
                    "codestral",
                    "codestral2",
                    "groq",
                    "gemini4",
                    "cohere",
                    "gemini",
                    "ollama"
                ]
            },
            "max_tokens": 2048,
            "temperature": 0.4
        },
        "ai3": {
            "providers": [
                "codestral",
                "groq",
                "together",
                "openrouter-qwen",
                "ollama"
            ],
            "model": "llama3.2:latest",
            "max_tokens": 2048,
            "temperature": 0.7
        }
    },
    "providers": {
        "ollama": {
            "type": "ollama",
            "endpoint": "http://host.docker.internal:11434",
            "model": "qwen3:latest",
            "api_key_env": "OLLAMA_API_KEY"
        },
        "ollama2": {
            "type": "ollama",
            "endpoint": "http://host.docker.internal:11434",
            "model": "llama3.2:latest",
            "api_key_env": "OLLAMA2_API_KEY"
        },
        "ollama3": {
            "type": "ollama",
            "endpoint": "http://46.219.108.236:11434",
            "model": "qwen2:1.5b",
            "api_key_env": "OLLAMA3_API_KEY"
        },
        "codestral": {
            "type": "codestral",
            "endpoint": "https://codestral.mistral.ai/v1",
            "model": "codestral-latest",
            "api_key_env": "CODESTRAL_API_KEY"
        },
        "codestral2": {
            "type": "codestral",
            "endpoint": "https://codestral.mistral.ai/v1",
            "model": "codestral-latest",
            "api_key_env": "CODESTRAL2_API_KEY"
        },
        "codestral3": {
            "type": "codestral",
            "endpoint": "https://codestral.mistral.ai/v1",
            "model": "codestral-latest",
            "api_key_env": "CODESTRAL3_API_KEY"
        },
        "gemini": {
            "type": "gemini",
            "model": "gemini-1.5-flash",
            "api_key_env": "GEMINI_API_KEY"
        },
        "gemini2": {
            "type": "gemini",
            "model": "gemini-1.5-flash",
            "api_key_env": "GEMINI2_API_KEY"
        },
        "gemini-pro2.5": {
            "type": "gemini",
            "model": "gemini-2.5-pro-preview-03-25",
            "api_key_env": "GEMINI_PRO25_API_KEY"
        },
        "gemini3": {
            "type": "gemini3",
            "model": "gemini-2.0-flash",
            "api_key_env": "GEMINI3_API_KEY"
        },
        "gemini4": {
            "type": "gemini4",
            "model": "gemini-2.0-pro",
            "api_key_env": "GEMINI4_API_KEY"
        },
        "cohere": {
            "type": "cohere",
            "endpoint": null,
            "model": "command-r",
            "api_key_env": "COHERE_API_KEY"
        },
        "cohere2": {
            "type": "cohere",
            "endpoint": null,
            "model": "command-r-plus",
            "api_key_env": "COHERE2_API_KEY"
        },
        "groq": {
            "type": "groq",
            "endpoint": null,
            "model": "llama3-70b-8192",
            "api_key_env": "GROQ_API_KEY"
        },
        "groq2": {
            "type": "groq",
            "endpoint": null,
            "model": "mixtral-8x7b-32768",
            "api_key_env": "GROQ2_API_KEY"
        },
        "together": {
            "type": "together",
            "endpoint": null,
            "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
            "api_key_env": "TOGETHER_API_KEY"
        },
        "together2": {
            "type": "together",
            "endpoint": null,
            "model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
            "api_key_env": "TOGETHER2_API_KEY"
        },
        "openrouter": {
            "type": "openrouter",
            "endpoint": "https://openrouter.ai/api/v1",
            "model": "anthropic/claude-3-opus",
            "max_tokens": 1024,
            "api_key_env": "OPENROUTER_API_KEY"
        },
        "openrouter2": {
            "type": "openrouter",
            "endpoint": "https://openrouter.ai/api/v1",
            "model": "anthropic/claude-3-sonnet",
            "max_tokens": 1024,
            "api_key_env": "OPENROUTER2_API_KEY"
        },
        "openrouter-qwen": {
            "type": "openrouter",
            "endpoint": "https://openrouter.ai/api/v1",
            "model": "qwen/qwen-2.5-coder-32b-instruct",
            "max_tokens": 1024,
            "api_key_env": "OPENROUTER_QWEN_API_KEY"
        },
        "openrouter-gemini": {
            "type": "openrouter",
            "endpoint": "https://openrouter.ai/api/v1",
            "model": "google/gemini-pro-1.5",
            "max_tokens": 1024,
            "api_key_env": "OPENROUTER_GEMINI_API_KEY"
        }
    },
    "request_delays": {
        "ai1": {
            "min": 0.1,
            "max": 0.5
        },
        "ai2": {
            "executor": {
                "min": 0.5,
                "max": 5.0
            },
            "tester": {
                "min": 0.5,
                "max": 5.0
            },
            "documenter": {
                "min": 0.5,
                "max": 8.5
            }
        },
        "ai3": {
            "min": 0.5,
            "max": 8.5
        }
    },
    "error_handling": {
        "quota_exceeded_retry_delay": 5,
        "max_provider_retries": 3,
        "fallback_enabled": true
    },
    "desired_active_buffer": 20,
    "ai1_desired_active_buffer": 5,
    "ai1_active_sleep_interval": 3,
    "ai1_pending_sleep_interval": 5,
    "ai1_idle_sleep_interval": 10,
    "github_repo": "oleg121203/AI-SYSTEMS-REPO",
    "github_actions_check_interval": 60
}